{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install huggingface_hub hf_transfer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update configuration\n",
    "please change the `ROLE` variable to a valid SageMaker execution role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROLE = \"arn:aws:iam::XXXX:role/service-role/AmazonSageMaker-ExecutionRole-20190405T234154\"\n",
    "HF_MODEL_ID = \"amazon/FalconLite2\"\n",
    "REPO_NAME = \"falconlite2-tgi103-ecr\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package FalconLight2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "\n",
    "model_tar_dir = Path(HF_MODEL_ID.split(\"/\")[-1])\n",
    "model_tar_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Download model from Hugging Face into model_dir\n",
    "if os.path.exists(model_tar_dir):\n",
    "    snapshot_download(\n",
    "        HF_MODEL_ID,\n",
    "        local_dir=str(model_tar_dir),\n",
    "        revision=\"main\",\n",
    "        local_dir_use_symlinks=False,\n",
    "        ignore_patterns=[\"*.msgpack*\", \"*.h5*\", \"*.bin*\"], # to load safetensor weights only\n",
    "    )\n",
    "\n",
    "# check if safetensor weights are downloaded and available\n",
    "assert len(list(model_tar_dir.glob(\"*.safetensors\"))) > 0, \"Model download failed\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    parent_dir=os.getcwd()\n",
    "    # change to model dir\n",
    "    os.chdir(str(model_tar_dir))\n",
    "    # use pigz for faster and parallel compression\n",
    "    !tar -cf model.tar.gz --use-compress-program=pigz *\n",
    "    # change back to parent dir\n",
    "    os.chdir(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    from sagemaker.s3 import S3Uploader\n",
    "    import sagemaker\n",
    "\n",
    "    sess = sagemaker.Session()\n",
    "\n",
    "    s3_model_uri = S3Uploader.upload(\n",
    "        local_path=str(model_tar_dir.joinpath(\"model.tar.gz\")),\n",
    "        desired_s3_uri=f\"s3://{sess.default_bucket()}/{model_tar_dir}\",\n",
    "    )\n",
    "\n",
    "    print(f\"model uploaded to: {s3_model_uri}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up the Docker build context\n",
    "Please manually move it to other directories if you do not wish to delete the model files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {model_tar_dir}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Custom Container\n",
    "First we augment the Dockerfile for SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_entry_stmt = \"\"\"\n",
    "\n",
    "COPY sagemaker-entrypoint.sh entrypoint.sh\n",
    "RUN chmod +x entrypoint.sh\n",
    "\n",
    "ENTRYPOINT [\"./entrypoint.sh\"]\n",
    "CMD [ \"\" ]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Dockerfile_rebuild_vllm_rope-theta\", \"r\") as fin:\n",
    "    docker_content = fin.read()\n",
    "\n",
    "sm_docker_cotent = docker_content + sm_entry_stmt\n",
    "\n",
    "with open(\"Dockerfile_sm\", \"w\") as fout:\n",
    "    fout.write(sm_docker_cotent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we build the image. This could take 10 minutes, so feel free to run it directly in the terminal in case the notebook cell times out.  \n",
    "\n",
    "**Important Note** - Please ensure the `ROLE` has sufficient permission to push Docker images to Elastic Container Registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash sm_build.sh {REPO_NAME}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "def get_aws_region():\n",
    "    # Get the current AWS region from the default session\n",
    "    session = boto3.session.Session()\n",
    "    return session.region_name\n",
    "\n",
    "def get_aws_account_id():\n",
    "    # Get the current AWS account ID from the default session\n",
    "    sts_client = boto3.client(\"sts\")\n",
    "    response = sts_client.get_caller_identity()\n",
    "    return response[\"Account\"]\n",
    "\n",
    "REGION = get_aws_region()\n",
    "ACCOUNT_ID = get_aws_account_id()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_image = f\"{ACCOUNT_ID}.dkr.ecr.{REGION}.amazonaws.com/{REPO_NAME}\"\n",
    "custom_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "\n",
    "print(f\"sagemaker role arn: {ROLE}\")\n",
    "print(f\"MODEL_S3_LOCATION: {s3_model_uri}\")\n",
    "\n",
    "instance_type = \"ml.g5.12xlarge\"\n",
    "num_gpu = 4\n",
    "\n",
    "health_check_timeout = 600\n",
    "\n",
    "max_input_length = 24000\n",
    "max_total_tokens = 24576\n",
    "\n",
    "config = {\n",
    "    \"HF_MODEL_ID\": \"/opt/ml/model\",\n",
    "    \"SM_NUM_GPUS\": json.dumps(num_gpu), \n",
    "    \"MAX_INPUT_LENGTH\": json.dumps(max_input_length),\n",
    "    \"MAX_TOTAL_TOKENS\": json.dumps(max_total_tokens),\n",
    "    \"HF_MODEL_QUANTIZE\": \"gptq\",\n",
    "    \"TRUST_REMOTE_CODE\": json.dumps(True),\n",
    "    \"MAX_BATCH_PREFILL_TOKENS\": json.dumps(max_input_length),\n",
    "}\n",
    "\n",
    "endpoint_name = sagemaker.utils.name_from_base(f\"falconlite2-g5-{num_gpu}gpu\")\n",
    "\n",
    "llm_model = HuggingFaceModel(\n",
    "    role=ROLE,\n",
    "    image_uri=custom_image,\n",
    "    env=config,\n",
    "    model_data=s3_model_uri\n",
    ")\n",
    "\n",
    "llm_model.deploy(\n",
    "    endpoint_name=endpoint_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    container_startup_health_check_timeout=health_check_timeout,\n",
    "    wait=False,\n",
    ")\n",
    "\n",
    "print(f\"Endpointname: {endpoint_name}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "def call_endpoint(text:str, endpoint_name:str):\n",
    "    client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "    parameters = {\n",
    "        \"max_new_tokens\": 250,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": None,\n",
    "        \"typical_p\": 0.2,\n",
    "        \"use_cache\": True,\n",
    "        \"seed\": 1,\n",
    "    }\n",
    "\n",
    "    payload = {\"inputs\": text, \"parameters\": parameters}\n",
    "\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, Body=json.dumps(payload), ContentType=\"application/json\"\n",
    "    )\n",
    "\n",
    "    output = json.loads(response[\"Body\"].read().decode())\n",
    "\n",
    "    result = output[0][\"generated_text\"]\n",
    "    return result\n",
    "\n",
    "\n",
    "prompt_template = \"<|prompter|>{text}<|endoftext|><|assistant|>\"\n",
    "text = \"What are the main challenges to support a long context for LLM?\"\n",
    "prompt = prompt_template.format(text=text)\n",
    "print(prompt)\n",
    "\n",
    "result = call_endpoint(prompt, endpoint_name)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the long context of over 13,400 tokens, which are copied from [Amazon Aurora FAQs](https://aws.amazon.com/rds/aurora/faqs/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with open(\"example_long_ctx.txt\", \"r\") as fin:\n",
    "    prompt = fin.read()\n",
    "prompt = prompt.format(\n",
    "    my_question=\"please tell me how does pgvector help with Generative AI and give me some examples.\"\n",
    ")\n",
    "prompt = prompt_template.format(text=prompt)\n",
    "result = call_endpoint(prompt, endpoint_name)\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inference",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
