{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d215e9d",
   "metadata": {},
   "source": [
    "# Deploying MultiModal Models with LMI Deep Learning Containers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d37f78b",
   "metadata": {},
   "source": [
    "In this tutorial, you will use the LMI DLC available on SageMaker to host and serve inference for a MultiModal model. We will be using the [Llava-v1.6](https://huggingface.co/llava-hf/llava-v1.6-mistral-7b-hf) model available on the HuggingFace Hub.\n",
    "\n",
    "Please make sure that you have an IAM role with SageMaker access enabled before proceeding with this example. \n",
    "\n",
    "For a list of supported multimodal models in LMI, please see the documentation [here]()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead96092",
   "metadata": {},
   "source": [
    "## Step 1: Install Notebook Python Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13375a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sagemaker --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dbad39-1862-42da-9523-627da14fe6ef",
   "metadata": {},
   "source": [
    "## Step 2: Build the LMI container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb999a2f-70ee-4471-8066-17dbbaef98f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone --branch v0.29.0 https://github.com/deepjavalibrary/djl-serving.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fda74d4-09b5-42f3-9ade-10d1b5bad46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile djl-serving/serving/docker/llava.py\n",
    "\n",
    "from typing import Iterable, List, Literal, Optional, Tuple, TypedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import CLIPVisionConfig, LlavaConfig\n",
    "\n",
    "from vllm.attention import AttentionMetadata\n",
    "from vllm.config import CacheConfig, MultiModalConfig\n",
    "from vllm.inputs import INPUT_REGISTRY, InputContext, LLMInputs\n",
    "from vllm.model_executor.layers.activation import get_act_fn\n",
    "from vllm.model_executor.layers.logits_processor import LogitsProcessor\n",
    "from vllm.model_executor.layers.quantization.base_config import (\n",
    "    QuantizationConfig)\n",
    "from vllm.model_executor.layers.sampler import Sampler\n",
    "from vllm.model_executor.layers.vocab_parallel_embedding import ParallelLMHead\n",
    "from vllm.model_executor.model_loader.weight_utils import default_weight_loader\n",
    "from vllm.model_executor.models.clip import CLIPVisionModel\n",
    "from vllm.model_executor.models.llama import LlamaModel\n",
    "from vllm.model_executor.models.qwen2 import Qwen2Model\n",
    "from vllm.model_executor.sampling_metadata import SamplingMetadata\n",
    "from vllm.multimodal import MULTIMODAL_REGISTRY\n",
    "from vllm.sequence import IntermediateTensors, SamplerOutput\n",
    "\n",
    "from .clip import (dummy_image_for_clip, dummy_seq_data_for_clip,\n",
    "                   get_max_clip_image_tokens, input_processor_for_clip)\n",
    "from .interfaces import SupportsVision\n",
    "from .utils import merge_vision_embeddings\n",
    "\n",
    "_KEYS_TO_MODIFY_MAPPING = {\n",
    "    \"language_model.lm_head\": \"lm_head\",\n",
    "    \"language_model.model\": \"language_model\",\n",
    "}\n",
    "\n",
    "def new_qwen2_forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        positions: torch.Tensor,\n",
    "        kv_caches: List[torch.Tensor],\n",
    "        attn_metadata: AttentionMetadata,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        if inputs_embeds is not None:\n",
    "            hidden_states = inputs_embeds\n",
    "        else:\n",
    "            hidden_states = self.get_input_embeddings(input_ids)\n",
    "        residual = None\n",
    "        for i in range(len(self.layers)):\n",
    "            layer = self.layers[i]\n",
    "            hidden_states, residual = layer(\n",
    "                positions,\n",
    "                hidden_states,\n",
    "                kv_caches[i],\n",
    "                attn_metadata,\n",
    "                residual,\n",
    "            )\n",
    "        hidden_states, _ = self.norm(hidden_states, residual)\n",
    "        return hidden_states\n",
    "\n",
    "def get_input_embeddings(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        return self.embed_tokens(input_ids)\n",
    "\n",
    "Qwen2Model.forward = (\n",
    "    new_qwen2_forward\n",
    ")\n",
    "Qwen2Model.get_input_embeddings = (\n",
    "    get_input_embeddings\n",
    ")\n",
    "\n",
    "# TODO(xwjiang): Run benchmark and decide if TP.\n",
    "class LlavaMultiModalProjector(nn.Module):\n",
    "\n",
    "    def __init__(self, vision_hidden_size: int, text_hidden_size: int,\n",
    "                 projector_hidden_act: str):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_1 = nn.Linear(vision_hidden_size,\n",
    "                                  text_hidden_size,\n",
    "                                  bias=True)\n",
    "        self.act = get_act_fn(projector_hidden_act)\n",
    "        self.linear_2 = nn.Linear(text_hidden_size,\n",
    "                                  text_hidden_size,\n",
    "                                  bias=True)\n",
    "\n",
    "    def forward(self, image_features: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.linear_1(image_features)\n",
    "        hidden_states = self.act(hidden_states)\n",
    "        hidden_states = self.linear_2(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class LlavaImagePixelInputs(TypedDict):\n",
    "    type: Literal[\"pixel_values\"]\n",
    "    data: torch.Tensor\n",
    "    \"\"\"Shape: `(batch_size, num_channels, height, width)`\"\"\"\n",
    "\n",
    "\n",
    "LlavaImageInputs = LlavaImagePixelInputs\n",
    "\n",
    "\n",
    "def get_max_llava_image_tokens(ctx: InputContext):\n",
    "    hf_config = ctx.get_hf_config(LlavaConfig)\n",
    "    vision_config = hf_config.vision_config\n",
    "\n",
    "    if isinstance(vision_config, CLIPVisionConfig):\n",
    "        return get_max_clip_image_tokens(vision_config)\n",
    "\n",
    "    msg = f\"Unsupported vision config: {type(vision_config)}\"\n",
    "    raise NotImplementedError(msg)\n",
    "\n",
    "\n",
    "def dummy_data_for_llava(ctx: InputContext, seq_len: int):\n",
    "    hf_config = ctx.get_hf_config(LlavaConfig)\n",
    "    vision_config = hf_config.vision_config\n",
    "\n",
    "    if isinstance(vision_config, CLIPVisionConfig):\n",
    "        seq_data = dummy_seq_data_for_clip(\n",
    "            vision_config,\n",
    "            seq_len,\n",
    "            image_token_id=hf_config.image_token_index,\n",
    "        )\n",
    "\n",
    "        mm_data = dummy_image_for_clip(vision_config)\n",
    "        return seq_data, mm_data\n",
    "\n",
    "    msg = f\"Unsupported vision config: {type(vision_config)}\"\n",
    "    raise NotImplementedError(msg)\n",
    "\n",
    "\n",
    "def input_processor_for_llava(ctx: InputContext, llm_inputs: LLMInputs):\n",
    "    multi_modal_data = llm_inputs.get(\"multi_modal_data\")\n",
    "    if multi_modal_data is None or \"image\" not in multi_modal_data:\n",
    "        return llm_inputs\n",
    "\n",
    "    model_config = ctx.model_config\n",
    "    hf_config = ctx.get_hf_config(LlavaConfig)\n",
    "    vision_config = hf_config.vision_config\n",
    "\n",
    "    if isinstance(vision_config, CLIPVisionConfig):\n",
    "        return input_processor_for_clip(\n",
    "            model_config,\n",
    "            vision_config,\n",
    "            llm_inputs,\n",
    "            image_token_id=hf_config.image_token_index,\n",
    "        )\n",
    "\n",
    "    msg = f\"Unsupported vision config: {type(vision_config)}\"\n",
    "    raise NotImplementedError(msg)\n",
    "\n",
    "@MULTIMODAL_REGISTRY.register_image_input_mapper()\n",
    "@MULTIMODAL_REGISTRY.register_max_image_tokens(get_max_llava_image_tokens)\n",
    "@INPUT_REGISTRY.register_dummy_data(dummy_data_for_llava)\n",
    "@INPUT_REGISTRY.register_input_processor(input_processor_for_llava)\n",
    "class LlavaForConditionalGeneration(nn.Module, SupportsVision):\n",
    "\n",
    "    def __init__(self,\n",
    "                 config: LlavaConfig,\n",
    "                 multimodal_config: MultiModalConfig,\n",
    "                 cache_config: Optional[CacheConfig] = None,\n",
    "                 quant_config: Optional[QuantizationConfig] = None) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "        self.multimodal_config = multimodal_config\n",
    "\n",
    "        # Initialize the vision tower only up to the required feature layer\n",
    "        vision_feature_layer = config.vision_feature_layer\n",
    "        if vision_feature_layer < 0:\n",
    "            num_hidden_layers = config.vision_config.num_hidden_layers \\\n",
    "                + vision_feature_layer + 1\n",
    "        else:\n",
    "            num_hidden_layers = vision_feature_layer + 1\n",
    "\n",
    "        # TODO: Optionally initializes this for supporting embeddings.\n",
    "        self.vision_tower = CLIPVisionModel(\n",
    "            config.vision_config, num_hidden_layers_override=num_hidden_layers)\n",
    "        self.multi_modal_projector = LlavaMultiModalProjector(\n",
    "            vision_hidden_size=config.vision_config.hidden_size,\n",
    "            text_hidden_size=config.text_config.hidden_size,\n",
    "            projector_hidden_act=config.projector_hidden_act)\n",
    "\n",
    "        self.quant_config = quant_config\n",
    "        ##### Change the original code to support qwen2 #####\n",
    "        if config.text_config.model_type == \"llama\":\n",
    "            self.language_model = LlamaModel(config.text_config, cache_config,\n",
    "                                         quant_config)\n",
    "        elif config.text_config.model_type == \"qwen2\":\n",
    "            self.language_model = Qwen2Model(config.text_config, cache_config,\n",
    "                                         quant_config)\n",
    "            self.language_model.org_vocab_size = config.text_config.vocab_size\n",
    "        else:\n",
    "            raise ValueError(f\"{config.text_config.model_type} is not supported by llava yet!!!\")\n",
    "        #####################################################\n",
    "        self.unpadded_vocab_size = config.text_config.vocab_size\n",
    "        self.lm_head = ParallelLMHead(\n",
    "            self.unpadded_vocab_size,\n",
    "            config.text_config.hidden_size,\n",
    "            org_num_embeddings=self.language_model.org_vocab_size,\n",
    "            quant_config=quant_config)\n",
    "        logit_scale = getattr(config, \"logit_scale\", 1.0)\n",
    "        self.logits_processor = LogitsProcessor(self.unpadded_vocab_size,\n",
    "                                                config.text_config.vocab_size,\n",
    "                                                logit_scale)\n",
    "        self.sampler = Sampler()\n",
    "\n",
    "    def _validate_pixel_values(self, data: torch.Tensor) -> torch.Tensor:\n",
    "        h = w = self.config.vision_config.image_size\n",
    "        expected_dims = (3, h, w)\n",
    "        actual_dims = tuple(data.shape[1:])\n",
    "\n",
    "        if actual_dims != expected_dims:\n",
    "            expected_expr = (\"batch_size\", *map(str, expected_dims))\n",
    "            raise ValueError(\n",
    "                f\"The expected shape of pixel values is {expected_expr}. \"\n",
    "                f\"You supplied {tuple(data.shape)}.\")\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _parse_and_validate_image_input(\n",
    "            self, **kwargs: object) -> Optional[LlavaImageInputs]:\n",
    "        pixel_values = kwargs.pop(\"pixel_values\", None)\n",
    "\n",
    "        if pixel_values is None:\n",
    "            return None\n",
    "\n",
    "        if not isinstance(pixel_values, torch.Tensor):\n",
    "            raise ValueError(\"Incorrect type of pixel values. \"\n",
    "                             f\"Got type: {type(pixel_values)}\")\n",
    "\n",
    "        return LlavaImagePixelInputs(\n",
    "            type=\"pixel_values\",\n",
    "            data=self._validate_pixel_values(pixel_values),\n",
    "        )\n",
    "\n",
    "    def _select_image_features(self, image_features: torch.Tensor, *,\n",
    "                               strategy: str) -> torch.Tensor:\n",
    "        # Copied from https://github.com/huggingface/transformers/blob/39c3c0a72af6fbda5614dde02ff236069bb79827/src/transformers/models/llava/modeling_llava.py#L421  # noqa\n",
    "        if strategy == \"default\":\n",
    "            return image_features[:, 1:]\n",
    "        elif strategy == \"full\":\n",
    "            return image_features\n",
    "\n",
    "        raise ValueError(f\"Unexpected select feature strategy: {strategy}\")\n",
    "\n",
    "    def _image_pixels_to_features(self, vision_tower: CLIPVisionModel,\n",
    "                                  pixel_values: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        # NOTE: we skip the step to select the vision feature layer since\n",
    "        # this is already done inside the vision tower\n",
    "        image_features = vision_tower(pixel_values)\n",
    "\n",
    "        return self._select_image_features(\n",
    "            image_features,\n",
    "            strategy=self.config.vision_feature_select_strategy,\n",
    "        )\n",
    "\n",
    "    def _process_image_pixels(self,\n",
    "                              inputs: LlavaImagePixelInputs) -> torch.Tensor:\n",
    "        assert self.vision_tower is not None\n",
    "\n",
    "        pixel_values = inputs[\"data\"]\n",
    "\n",
    "        return self._image_pixels_to_features(self.vision_tower, pixel_values)\n",
    "\n",
    "    def _process_image_input(self,\n",
    "                             image_input: LlavaImageInputs) -> torch.Tensor:\n",
    "        assert self.vision_tower is not None\n",
    "        image_features = self._process_image_pixels(image_input)\n",
    "        return self.multi_modal_projector(image_features)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        positions: torch.Tensor,\n",
    "        kv_caches: List[torch.Tensor],\n",
    "        attn_metadata: AttentionMetadata,\n",
    "        intermediate_tensors: Optional[IntermediateTensors] = None,\n",
    "        **kwargs: object,\n",
    "    ) -> SamplerOutput:\n",
    "        \"\"\"Run forward pass for LLaVA-1.5.\n",
    "\n",
    "        One key thing to understand is the `input_ids` already accounts for the\n",
    "        positions of the to-be-inserted image embeddings.\n",
    "\n",
    "        Concretely, consider a text prompt:\n",
    "        `\"USER: <image>\\\\nWhat's the content of the image?\\\\nASSISTANT:\"`.\n",
    "\n",
    "        Tokenizer outputs:\n",
    "        `[1, 3148, 1001, 29901, 29871, 32000, 29871, 13, 5618, 29915, 29879,\n",
    "        278, 2793, 310, 278, 1967, 29973, 13, 22933, 9047, 13566, 29901]`.\n",
    "\n",
    "        To reserve space in KV cache, we have to insert placeholder tokens\n",
    "        before they are inputted to the model, so the input processor prepends \n",
    "        additional image tokens (denoted as `32000`), resulting in:\n",
    "        `[1, 3148, 1001, 29901, 29871, 32000, ..., 32000, 29871, 13, 5618,\n",
    "        29915, 29879, 278, 2793, 310, 278, 1967, 29973, 13, 22933, 9047, 13566,\n",
    "        29901]`.\n",
    "\n",
    "        We insert 575 tokens so that including the original image token in the\n",
    "        input, there are a total of 576 (24 * 24) image tokens, which\n",
    "        corresponds to the number of image tokens inputted to the language\n",
    "        model, i.e. the number of image tokens outputted by the visual encoder.\n",
    "\n",
    "        This way, the `positions` and `attn_metadata` are consistent\n",
    "        with the `input_ids`.\n",
    "\n",
    "        Args:\n",
    "            input_ids: Flattened (concatenated) input_ids corresponding to a\n",
    "                batch.\n",
    "            pixel_values: The pixels in each input image.\n",
    "        \n",
    "        See also:\n",
    "            :class:`LlavaImageInputs`\n",
    "        \"\"\"\n",
    "        image_input = self._parse_and_validate_image_input(**kwargs)\n",
    "\n",
    "        if image_input is not None:\n",
    "            vision_embeddings = self._process_image_input(image_input)\n",
    "            inputs_embeds = self.language_model.get_input_embeddings(input_ids)\n",
    "\n",
    "            inputs_embeds = merge_vision_embeddings(\n",
    "                input_ids, inputs_embeds, vision_embeddings,\n",
    "                self.config.image_token_index)\n",
    "\n",
    "            input_ids = None\n",
    "        else:\n",
    "            inputs_embeds = None\n",
    "\n",
    "\n",
    "        if self.config.text_config.model_type == \"llama\":\n",
    "            hidden_states = self.language_model(input_ids,\n",
    "                                            positions,\n",
    "                                            kv_caches,\n",
    "                                            attn_metadata,\n",
    "                                            None,\n",
    "                                            inputs_embeds=inputs_embeds)\n",
    "        elif self.config.text_config.model_type == \"qwen2\":\n",
    "            hidden_states = self.language_model(input_ids,\n",
    "                                            positions,\n",
    "                                            kv_caches,\n",
    "                                            attn_metadata,\n",
    "                                            inputs_embeds=inputs_embeds)\n",
    "        else:\n",
    "            raise ValueError(f\"{config.text_config.model_type} is not supported by llava yet!!!\")\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "    def compute_logits(self, hidden_states: torch.Tensor,\n",
    "                       sampling_metadata: SamplingMetadata) -> torch.Tensor:\n",
    "        logits = self.logits_processor(self.lm_head, hidden_states,\n",
    "                                       sampling_metadata)\n",
    "        return logits\n",
    "\n",
    "    def sample(\n",
    "        self,\n",
    "        logits: torch.Tensor,\n",
    "        sampling_metadata: SamplingMetadata,\n",
    "    ) -> Optional[SamplerOutput]:\n",
    "        next_tokens = self.sampler(logits, sampling_metadata)\n",
    "        return next_tokens\n",
    "\n",
    "    def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]]):\n",
    "        # only doing this for language model part for now.\n",
    "        stacked_params_mapping = [\n",
    "            # (param_name, shard_name, shard_id)\n",
    "            (\"qkv_proj\", \"q_proj\", \"q\"),\n",
    "            (\"qkv_proj\", \"k_proj\", \"k\"),\n",
    "            (\"qkv_proj\", \"v_proj\", \"v\"),\n",
    "            (\"gate_up_proj\", \"gate_proj\", 0),\n",
    "            (\"gate_up_proj\", \"up_proj\", 1),\n",
    "        ]\n",
    "        params_dict = dict(self.named_parameters())\n",
    "        for name, loaded_weight in weights:\n",
    "            if \"rotary_emb.inv_freq\" in name:\n",
    "                continue\n",
    "            # post_layernorm is not needed in CLIPVisionModel\n",
    "            if \"vision_model.post_layernorm\" in name:\n",
    "                continue\n",
    "            for key_to_modify, new_key in _KEYS_TO_MODIFY_MAPPING.items():\n",
    "                if key_to_modify in name:\n",
    "                    name = name.replace(key_to_modify, new_key)\n",
    "            use_default_weight_loading = False\n",
    "            if \"vision\" in name:\n",
    "                if self.vision_tower is not None:\n",
    "                    # We only do sharding for language model and\n",
    "                    # not vision model for now.\n",
    "                    use_default_weight_loading = True\n",
    "            else:\n",
    "                for (param_name, weight_name,\n",
    "                     shard_id) in stacked_params_mapping:\n",
    "                    if weight_name not in name:\n",
    "                        continue\n",
    "                    param = params_dict[name.replace(weight_name, param_name)]\n",
    "                    weight_loader = param.weight_loader\n",
    "                    weight_loader(param, loaded_weight, shard_id)\n",
    "                    break\n",
    "                else:\n",
    "                    use_default_weight_loading = True\n",
    "            if use_default_weight_loading and name in params_dict:\n",
    "                param = params_dict[name]\n",
    "                weight_loader = getattr(param, \"weight_loader\",\n",
    "                                        default_weight_loader)\n",
    "                weight_loader(param, loaded_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29446bad-e64d-4276-b50f-796ad5022be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile djl-serving/serving/docker/lmi.Dockerfile\n",
    "\n",
    "# -*- mode: dockerfile -*-\n",
    "# Copyright 2022 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file\n",
    "# except in compliance with the License. A copy of the License is located at\n",
    "#\n",
    "# http://aws.amazon.com/apache2.0/\n",
    "#\n",
    "# or in the \"LICENSE.txt\" file accompanying this file. This file is distributed on an \"AS IS\"\n",
    "# BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, express or implied. See the License for\n",
    "# the specific language governing permissions and limitations under the License.\n",
    "ARG version=12.4.1-cudnn-devel-ubuntu22.04\n",
    "FROM nvidia/cuda:$version\n",
    "ARG cuda_version=cu124\n",
    "ARG djl_version=0.29.0~SNAPSHOT\n",
    "# Base Deps\n",
    "ARG python_version=3.10\n",
    "ARG torch_version=2.3.1\n",
    "ARG torch_vision_version=0.18.1\n",
    "ARG onnx_version=1.18.0\n",
    "ARG onnxruntime_wheel=\"https://publish.djl.ai/onnxruntime/1.18.0/onnxruntime_gpu-1.18.0-cp310-cp310-linux_x86_64.whl\"\n",
    "ARG pydantic_version=2.8.2\n",
    "ARG djl_converter_wheel=\"https://publish.djl.ai/djl_converter/djl_converter-0.28.0-py3-none-any.whl\"\n",
    "# HF Deps\n",
    "ARG protobuf_version=3.20.3\n",
    "ARG transformers_version=4.43.2\n",
    "ARG accelerate_version=0.32.1\n",
    "ARG bitsandbytes_version=0.43.1\n",
    "ARG optimum_version=1.21.2\n",
    "ARG auto_gptq_version=0.7.1\n",
    "ARG datasets_version=2.20.0\n",
    "ARG autoawq_version=0.2.5\n",
    "ARG tokenizers_version=0.19.1\n",
    "# LMI-Dist Deps\n",
    "ARG vllm_wheel=\"https://publish.djl.ai/vllm/cu124-pt231/vllm-0.5.3.post1%2Bcu124-cp310-cp310-linux_x86_64.whl\"\n",
    "ARG flash_attn_2_wheel=\"https://github.com/vllm-project/flash-attention/releases/download/v2.5.9.post1/vllm_flash_attn-2.5.9.post1-cp310-cp310-manylinux1_x86_64.whl\"\n",
    "ARG flash_infer_wheel=\"https://github.com/flashinfer-ai/flashinfer/releases/download/v0.0.9/flashinfer-0.0.9+cu121torch2.3-cp310-cp310-linux_x86_64.whl\"\n",
    "# %2B is the url escape for the '+' character\n",
    "ARG lmi_dist_wheel=\"https://publish.djl.ai/lmi_dist/lmi_dist-11.0.0-py3-none-any.whl\"\n",
    "ARG seq_scheduler_wheel=\"https://publish.djl.ai/seq_scheduler/seq_scheduler-0.1.0-py3-none-any.whl\"\n",
    "ARG peft_version=0.11.1\n",
    "\n",
    "EXPOSE 8080\n",
    "\n",
    "COPY dockerd-entrypoint-with-cuda-compat.sh /usr/local/bin/dockerd-entrypoint.sh\n",
    "RUN chmod +x /usr/local/bin/dockerd-entrypoint.sh\n",
    "WORKDIR /opt/djl\n",
    "ENV JAVA_HOME=/usr/lib/jvm/java-17-amazon-corretto\n",
    "# ENV NO_OMP_NUM_THREADS=true\n",
    "ENV JAVA_OPTS=\"-Xmx1g -Xms1g -XX:+ExitOnOutOfMemoryError -Dai.djl.util.cuda.fork=true\"\n",
    "ENV MODEL_SERVER_HOME=/opt/djl\n",
    "ENV MODEL_LOADING_TIMEOUT=1200\n",
    "ENV PREDICT_TIMEOUT=240\n",
    "ENV DJL_CACHE_DIR=/tmp/.djl.ai\n",
    "ENV PYTORCH_LIBRARY_PATH=/usr/local/lib/python3.10/dist-packages/torch/lib\n",
    "ENV PYTORCH_PRECXX11=true\n",
    "ENV PYTORCH_VERSION=${torch_version}\n",
    "ENV PYTORCH_FLAVOR=cu121-precxx11\n",
    "ENV VLLM_NO_USAGE_STATS=1\n",
    "ENV VLLM_WORKER_MULTIPROC_METHOD=spawn\n",
    "\n",
    "\n",
    "ENV HF_HOME=/tmp/.cache/huggingface\n",
    "ENV PYTORCH_KERNEL_CACHE_PATH=/tmp/.cache\n",
    "ENV BITSANDBYTES_NOWELCOME=1\n",
    "ENV USE_AICCL_BACKEND=true\n",
    "ENV HF_HUB_ENABLE_HF_TRANSFER=1\n",
    "ENV SAFETENSORS_FAST_GPU=1\n",
    "ENV TORCH_NCCL_BLOCKING_WAIT=0\n",
    "ENV NCCL_ASYNC_ERROR_HANDLING=1\n",
    "ENV TORCH_NCCL_AVOID_RECORD_STREAMS=1\n",
    "ENV SERVING_FEATURES=vllm,lmi-dist\n",
    "\n",
    "ENTRYPOINT [\"/usr/local/bin/dockerd-entrypoint.sh\"]\n",
    "CMD [\"serve\"]\n",
    "\n",
    "COPY scripts scripts/\n",
    "RUN mkdir -p /opt/djl/conf \\\n",
    "    && mkdir -p /opt/djl/deps \\\n",
    "    && mkdir -p /opt/djl/partition \\\n",
    "    && mkdir -p /opt/ml/model\n",
    "COPY config.properties /opt/djl/conf/config.properties\n",
    "COPY partition /opt/djl/partition\n",
    "\n",
    "#COPY distribution[s]/ ./\n",
    "#RUN mv *.deb djl-serving_all.deb || true\n",
    "\n",
    "RUN apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -yq libaio-dev libopenmpi-dev g++ \\\n",
    "    && scripts/install_openssh.sh \\\n",
    "    && scripts/install_djl_serving.sh $djl_version \\\n",
    "    && scripts/install_djl_serving.sh $djl_version ${torch_version} \\\n",
    "    && djl-serving -i ai.djl.onnxruntime:onnxruntime-engine:$djl_version \\\n",
    "    && djl-serving -i com.microsoft.onnxruntime:onnxruntime_gpu:$onnx_version \\\n",
    "    && scripts/install_python.sh ${python_version} \\\n",
    "    && scripts/install_s5cmd.sh x64 \\\n",
    "    && mkdir -p /opt/djl/bin && cp scripts/telemetry.sh /opt/djl/bin \\\n",
    "    && echo \"${djl_version} lmi\" > /opt/djl/bin/telemetry \\\n",
    "    && pip3 cache purge \\\n",
    "    && apt-get clean -y && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "RUN pip3 install torch==${torch_version} torchvision==${torch_vision_version} --extra-index-url https://download.pytorch.org/whl/cu121 \\\n",
    "    ${seq_scheduler_wheel} peft==${peft_version} protobuf==${protobuf_version} \\\n",
    "    transformers==${transformers_version} hf-transfer zstandard datasets==${datasets_version} \\\n",
    "    mpi4py sentencepiece tiktoken blobfile einops accelerate==${accelerate_version} bitsandbytes==${bitsandbytes_version} \\\n",
    "    auto-gptq==${auto_gptq_version} pandas pyarrow jinja2 retrying \\\n",
    "    opencv-contrib-python-headless safetensors scipy onnx sentence_transformers ${onnxruntime_wheel} autoawq==${autoawq_version} \\\n",
    "    tokenizers==${tokenizers_version} pydantic==${pydantic_version} \\\n",
    "    # TODO: installing optimum here due to version conflict.\n",
    "    && pip3 install ${djl_converter_wheel} optimum==${optimum_version} --no-deps \\\n",
    "    && git clone https://github.com/neuralmagic/AutoFP8.git && cd AutoFP8 && git reset --hard 4b2092c && pip3 install . && cd .. && rm -rf AutoFP8 \\\n",
    "    && pip3 cache purge\n",
    "\n",
    "RUN pip3 install ${flash_attn_2_wheel} ${lmi_dist_wheel} ${vllm_wheel} ${flash_infer_wheel} \\\n",
    "    && pip3 cache purge\n",
    "COPY llava.py /usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/llava.py\n",
    "\n",
    "# Add CUDA-Compat\n",
    "RUN apt-get update && apt-get install -y cuda-compat-12-4 && apt-get clean -y && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "RUN scripts/patch_oss_dlc.sh python \\\n",
    "    && scripts/security_patch.sh lmi \\\n",
    "    && useradd -m -d /home/djl djl \\\n",
    "    && chown -R djl:djl /opt/djl \\\n",
    "    && rm -rf scripts \\\n",
    "    && pip3 cache purge \\\n",
    "    && apt-get clean -y && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "LABEL maintainer=\"djl-dev@amazon.com\"\n",
    "LABEL dlc_major_version=\"1\"\n",
    "LABEL com.amazonaws.ml.engines.sagemaker.dlc.framework.djl.lmi=\"true\"\n",
    "LABEL com.amazonaws.ml.engines.sagemaker.dlc.framework.djl.v0-29-0.lmi=\"true\"\n",
    "LABEL com.amazonaws.sagemaker.capabilities.multi-models=\"true\"\n",
    "LABEL com.amazonaws.sagemaker.capabilities.accept-bind-to-port=\"true\"\n",
    "LABEL djl-version=$djl_version\n",
    "LABEL cuda-version=$cuda_version\n",
    "# To use the 535 CUDA driver, CUDA 12.1 can work on this one too\n",
    "LABEL com.amazonaws.sagemaker.inference.cuda.verified_versions=12.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae15c8fa-cb4e-4fcd-9e85-8d560dfae3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sm_build.sh\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "REPO_NAME=${1:-custom-aws-lmi-ecr}\n",
    "DJL_VERSION=${3:-0.29.0}\n",
    "ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)\n",
    "REGION=$(aws configure get region)\n",
    "\n",
    "echo \"REPO_NAME: ${REPO_NAME}\"\n",
    "echo \"REGION: ${REGION}\"\n",
    "echo \"ACCOUNT_ID: ${ACCOUNT_ID}\"\n",
    "\n",
    "aws ecr get-login-password --region $REGION | docker login --username AWS --password-stdin $ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com\n",
    "\n",
    "aws ecr describe-repositories --repository-names ${REPO_NAME} --region $REGION\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    echo \"Creating ECR repository: ${REPO_NAME}\"\n",
    "    aws ecr create-repository --repository-name $REPO_NAME --region $REGION\n",
    "fi\n",
    "\n",
    "cd djl-serving/serving/docker\n",
    "\n",
    "docker build --build-arg djl_version=${DJL_VERSION} -f lmi.Dockerfile  -t $REPO_NAME .\n",
    "\n",
    "docker tag $REPO_NAME:latest $ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com/$REPO_NAME:latest\n",
    "\n",
    "docker push $ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com/$REPO_NAME:latest\n",
    "\n",
    "echo \"Container URI:\"\n",
    "echo \"$ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com/$REPO_NAME:latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da89c658-d439-4205-a202-ef219424ac38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!bash sm_build.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bff4b3",
   "metadata": {},
   "source": [
    "## Step 3: Leverage the SageMaker PythonSDK to deploy your endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de290482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.djl_inference import DJLModel\n",
    "from sagemaker import image_uris\n",
    "\n",
    "role = sagemaker.get_execution_role() # iam role for the endpoint\n",
    "session = sagemaker.session.Session() # sagemaker session for interacting with aws APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1b6c36-b496-4f8a-9465-7632407eb333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "def get_aws_region():\n",
    "    # Get the current AWS region from the default session\n",
    "    session = boto3.session.Session()\n",
    "    return session.region_name\n",
    "\n",
    "def get_aws_account_id():\n",
    "    # Get the current AWS account ID from the default session\n",
    "    sts_client = boto3.client(\"sts\")\n",
    "    response = sts_client.get_caller_identity()\n",
    "    return response[\"Account\"]\n",
    "\n",
    "REGION = get_aws_region()\n",
    "ACCOUNT_ID = get_aws_account_id()\n",
    "REPO_NAME=\"custom-aws-lmi-ecr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f524fed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the customized version of LMI image directly:\n",
    "image_uri = f\"{ACCOUNT_ID}.dkr.ecr.{REGION}.amazonaws.com/{REPO_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c5f812",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DJLModel(\n",
    "    model_id=\"aws-prototyping/long-llava-qwen2-7b\",\n",
    "    role=role,\n",
    "    image_uri=image_uri,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f63dd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = model.deploy(initial_instance_count=1, instance_type=\"ml.g5.4xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca76e2d",
   "metadata": {},
   "source": [
    "## Step 4: Make Inference Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cddff26",
   "metadata": {},
   "source": [
    "For multimodal models, LMI containers support the [OpenAI Chat Completions Schema](https://platform.openai.com/docs/guides/chat-completions). You can find specific details about LMI's implementation of this spec [here](https://docs.djl.ai/docs/serving/serving/docs/lmi/user_guides/chat_input_output_schema.html).\n",
    "\n",
    "The OpenAI Chat Completions Schema allows two methods of specifying the image data:\n",
    "\n",
    "* an image url (e.g. https://resources.djl.ai/images/dog_bike_car.jpg)\n",
    "* base64 encoded string of the image data\n",
    "\n",
    "If an image url is provided, the container will make a network call to fetch the image. This is ok for small applications and experimentation, but is not recommended in a production setting. If you are in a network isolated environment you must use the base64 encoded string representation.\n",
    "\n",
    "We will demonstrate both mechanisms here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ffd634",
   "metadata": {},
   "source": [
    "### Getting a Test Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3e833e",
   "metadata": {},
   "source": [
    "You are free to use any image that you want. In this example, we'll be using the following image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a75789",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4719217b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from PIL import Image\n",
    "\n",
    "image_url = \"https://resources.djl.ai/images/dog_bike_car.jpg\"\n",
    "#image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n",
    "image_path = \"dog_bike_car.jpg\"\n",
    "# download the image locally\n",
    "urllib.request.urlretrieve(image_url, image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60205c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('dog_bike_car.jpg')\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc49c44",
   "metadata": {},
   "source": [
    "### Using the image http url directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da8ded3",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"What is this image about?\"\n",
    "                }, \n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": image_url\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"seed\": 1000, \n",
    "    \"max_tokens\": 100,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c150722b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = predictor.predict(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c616bd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c13d2d-89f3-48f5-83a4-2b503184e052",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6726896",
   "metadata": {},
   "source": [
    "## Using the base64 encoded image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9cb2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "def encode_image_base64(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    \n",
    "encoded_image = encode_image_base64(image_path)\n",
    "base64_image_url = f\"data:image/jpeg;base64,{encoded_image}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbbc253",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"What is this image about?\"\n",
    "                }, \n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": base64_image_url\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "     \"seed\": 1000, \n",
    "    \"max_tokens\": 100,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4ba4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = predictor.predict(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dc9352",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4ed2e7-02fb-4fbd-a3ce-7c69ee72e224",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b428bc6d",
   "metadata": {},
   "source": [
    "## Clean Up Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccc5c7b",
   "metadata": {},
   "source": [
    "This example demonstrates how to use the LMI container to deploy MultiModal models and serve inference requests. The 0.29.0-lmi container supports a variety of multimodal models using the OpenAI Chat Completions API spec. In the future, we plan to increase the set of multimodal architectures supported, as well as provide additional API specs that can be used to make inference requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75481491",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()\n",
    "model.delete_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fea0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete downloaded repo, image, and script\n",
    "%rm -rf djl-serving\n",
    "%rm dog_bike_car.jpg\n",
    "%rm sm_build.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
